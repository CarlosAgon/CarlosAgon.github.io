---
layout: single
permalink: /atiam-ml-9-markov-models/
author_profile: false
share: true
comments: true
sidebar:
  nav: "teaching-atiam-ml"
---

{% include toc %}

# Hidden Markov Models

The present tutorials covers .

# Reference slides

Download the [slides ![](../images/pdf.png)](../documents/MML.Lesson.9.Hidden.Markov.Models.pdf)

The corresponding slides cover

  - Undirected graphical models
  - Hidden Markov models  
  
# Tutorial 

#### 9.0 - Introduction

<img src="pics/hmm-example.png" width="300px"/>
<div align="center">*Figure 3: A Hidden Markov Model, resulting from connecting several of the Bayes Net above in a sequence*</div>

Hidden Markov Models (HMMs) are used to model the changes over time (or another dimension), in which different elements might be connected. To capture this, we can include conditional probabilities, expressing how a random variable changes over time. In this model, we assume that each state depends only on the previous one, not all previous states, which is called the ***Markov property*** and can be written as

$$
\begin{equation}
P\left(x_{t} \mid x_{t - 1}, x_{t - 2}, \dots x_{2},x_{1}\right) = P\left(x_{t} \mid x_{t - 1}\right)
\end{equation}
$$

Hence, the model can be represented as a **Graphical Probabilistic Model** (GPM), in which the connections (arcs) between different states (nodes) are weighted by the probability of going to one state. This type of model is simply called a **Markov chain**. In order to make it a **Hidden** Markov Model, we assume that there is some observable variables but in fact, the real transitions come from a random variable that we are actually interested in but is not observable directly.

Based on this model, several tasks can be performed 

In most applications, a single hidden layer is sufficient to model the behavior of a system. There are several algorithms to do the inference in a HMM, but we will be using the ***Forward-Backward*** (or Baum-Welch) algorithm for labeling.

What we want in the end is the ***model parameters*** of our grpahical model (the conditional probability tables) that best explain the observed data. The configuration we want is the one which **maximizes the likelihood** of observing our data given one set of parameters for the model (the global maxima). Hence, we model a joint probability distribution  $$P(\mathbf{o}, \mathbf{h})$$., which represents the probability of an observation $$\mathbf{o}$$ and a hidden variable $$\mathbf{h}$$ sequence occurring together

$$
\begin{equation}
P(\mathbf{o},\mathbf{h}) = P(\mathbf{h}) \times P(\mathbf{o}\mid\mathbf{h}) = P(\mathbf{o}) \times P(\mathbf{h}\mid\mathbf{o})
\end{equation}
$$

This means that the probability of seeing an observation and hidden sequence together $$P(\mathbf{o}, \mathbf{o})$$ is equal to the probability of the hidden sequence $$P(\mathbf{h})$$ times the probability that we generate these observations given the hidden sequence
$$P(\mathbf{o}\mid\mathbf{h})$$. It is also equal to the probability of seeing those observations $$P(\mathbf{o})$$ times the probability of turning those observations into that hidden sequence $$P(\mathbf{h}\mid\mathbf{o})$$.

What we want to maximize is the last part, $$P(\mathbf{h}\mid\mathbf{o})$$ (i.e., what is the best hidden sequence for the observations we see), so we can rewrite the previous equation as

$$
\begin{equation}
\frac{P(\mathbf{h}) \times P(\mathbf{o}\mid\mathbf{h})}{P(\mathbf{o})} = P(\mathbf{h}\mid\mathbf{o})
\end{equation}
$$

Since we observe $$\mathbf{o}$$, we know that $$P(\mathbf{o}) = 1.0$$, so we can simply write

$$P(\mathbf{h}\mid\mathbf{o}) = P(\mathbf{h}) \times P(\mathbf{o}\mid\mathbf{h})$$

If we now consider $$P(\mathbf{h})$$, it is a sequence of hidden states, where each depends on the previous one. Because of the Markov property, the probability of seeing the whole sequence $$h_{1}, h_{2}, \cdots h_{n}$$ (i.e., $$P(\mathbf{h})$$) is just the product of seeing each of the states following another state. We can write that as

$$
\begin{equation}
P(\mathbf{h}) = P(h_{1}) \times P(h_{2} \mid h_{1}) \times P(h_{3} \mid h_{2}) \times \cdots \times P(h_{n} \mid h_{n-1})
\end{equation}
$$

or for short

$$
\begin{equation}
P(\mathbf{h}) = P(h_{1}) \times \prod^{n}_{2} P(h_{i} \mid h_{i-1})
\end{equation}
$$

Hence, the complete optimization of all parameters can be expressed as the product of our two parameters.

$$ 
\begin{equation}
P(\mathbf{h} \mid \mathbf{o}) = P(h_{1}) \times P(o_{1} \mid h_{1}) \times \prod^{n}_{2} P(t_{i} \mid t_{i-1}) \times P(w_{i} \mid t_{i})
\end{equation}
$$

In order to compute the parameters, we have to develop a data structure that allows us to manipulate
them. We will use a HMM to represent the model and ***lattice-based dynamic programming*** to compute and manipulate the probabilities. 

#### 9.1 - HMM as Lattice

The idea behind HMM is that what we can see (obervations) was generated by something we cannot see (the hidden state), which is the ***generative interpretation***. Observations are connected by ***transition probabilities*** $$P(h_{i} \mid h_{i-1})$$, and emitted observations with ***emission probabilities*** $$P(o_{i} \mid h_{i})$$. We can translate this to a sequence of conditional probabilities

$$ 
\begin{equation}
P(\mathbf{h} \mid \mathbf{o}) = P(h_{1}) \times P(o_{1} \mid h_{1}) \times \prod^{n}_{2} P(h_{i} \mid h_{i-1}) \times P(o_{i} \mid h_{i})
\end{equation}
$$

We can model HMMs as a ***lattice*** (matrix) of hidden states and observations, by replacing each random variable in the HMM with all possible values and drawing all possible arcs between them. It is important to specify all the hidden states you want to use. We start from a designated start state and from there choose one of the tags with the respective probability $$P(h)$$. From each of those possible tag states, we can emit an observation with the respective probability $$P(o \mid h)$$. Then, we choose the next hidden state with some probability $$P(h_{i} \mid h_{i-1})$$.

Ultimately, we want to learn which hidden states follow one another, and which observations correspond to those sequences. Hence, as previously in this course, we need to reward good parameters (transitions that increase $P(sequence)$) and we decrease bad ones. As a first step, instead of just taking whole counts of how often we see a transition, we "weigh" them by how likely the resulting sequence
was ($P(sequence)$). This is called ***fractional counts***.

** Handling underflows **
Since we multiply probabilistic transitions, the numbers can quickly become very small and lead to an underflow (numbers too small to be handled numerically). To deal with this, we can use the logarithm of the probabilities as the smaller a number gets, the larger its negative log will be. Furthermore, if we use logarithms, all multiplications shown here become additions, and all additions have to be log-additions, a special computation that unfortunately is quite slow.

#### 9.2 - Dynamic Programming

If we decompose our original question of finding $$P(o \mid h)$$, we can see that this turns to ask how likely it is that we end up at the node that has $$P(o \mid h)$$ as outgoing transition. And once we took that transition, what is the probability from the node we reach to the end of the sequence. By using dynamic programming, we can compute how likely it is to arrive at each node (with the Forward algorithm), and to get to the end from there (with the Backward algorithm).

We use Forward-Backward in order to efficiently compute for each sequence how often we see each transition and what the probability of that sequence is. We need both for the fractional counts. Forward-Backward is the E step in an EM implementation: we compute the expected counts given the current model parameters.

** The Forward Algorithm **

In the forward pass, we compute a new lattice with the same dimensions as the original one, which forward pass contains for each node the sum of all possible paths that lead up to there (see Figure 10). These values are also called alphas. $\alpha[i, j]$ denotes the probability of all paths up to node $$(i, j)$$ (i.e., assigning tag $$i$$ to word $$j$$). 

$$\alpha[START]$$ is always $1.0$. Each subsequent $\alpha$ is just the sum of all transitions arriving there, each multiplied by the $\alpha$ of the node where it (the transition) originated.

$\alpha[END]$ is the sum of all paths through the lattice, which is equal to $P(sentence)$. $P(data)$ is the
sum of all $P(sentence)$ in the data. In each iteration, just add up all the $\alpha[END]$ of the sentences.
Remember, $P(data)$ has to increase with each iteration, or there is something wrong! EM guarantees
that the likelihood of the data increases at each iteration over the data. Outputting $P(data)$ is thus a
good way of debugging your code: if it does not increase, something went wrong...

**Exercise**
<div markdown = "1">

  1. Implement the forward algorithm
  2. Debug it

</div>

**The Backward Algorithm**
<img src="pics/betalattice.png" width="600px"/>
<div align="center">*Figure 11: Computing the betas in the Backward pass*</div>

The backward pass is almost the same as the forward pass, just backwards (note how the direction of backward
the arrows is reversed in Figure 11). Again, we compute a new lattice, which contains for each node pass
the sum of all possible paths that lead from that node to the end. These values are called ***betas***. $\beta[i, j]$
denotes the summed probability of all paths from node $(i, j)$ to the end. This time, however, we start at
the end. $\beta[END]$ is always $1.0$.

A useful property for debugging is the fact that $\beta[START] = \alpha[END]$.


**Exercise**
<div markdown = "1">

  1. Implement the forward algorithm
  2. Debug it

</div>

**Collecting fractional counts**
<img src="pics/fraccount.png" width="300px"/>
<div align="center">*Figure 12: Collecting fractional counts for $P($ `like` | `V` $)$*</div>

Once we have the alphas and betas, it is easy to compute for each transition how much it contributes
to $P(sentence)$. So, once more, with conviction: how good is $P($ `like` | `V` $)$? Remember, we have to know
the likelihood of all possible paths arriving at node $(2,3)$, and the probability – once we have taken the
transition – from node $(2,4)$ to the end. See Figure 12.

We used the forward algorithm to get the probability of arriving at node $(2,3)$, and the backward
algorithm to compute how likely it is from node $(2,4)$ to the end. We divide that by the likelihood of
the sentence ($= \alpha[END]$), et voila!

**The M Step**

Computing alphas and betas and collecting the fractional counts for all free parameter transitions over
all examples is the ***E step***. This, as the name suggests, is one half of Forward-Backward EM, and in 
this case the bigger half. The ***M step*** is comparatively trivial: after having gone through all the data, we 
just normalize our fractional counts to get probabilities back (remember, probabilities are just normalized
counts).

#### References

If you want to read the original, go for [Dempster/Laird/Rubin (1977)](#refDempster).
[Rabiner/Juang (1986)](#refRabiner) is a general overview over ***parameter estimation***, but very math-heavy. [Manning/Schütze (2000)](#refMS) has a chapter on EM, based on clustering, but with an eye on other NLP applications.
One of the most famous implementations for NLP is the tagging paper by [Merialdo (1994)](refMerialdo),
which also gives you a good idea about the various parameters you can set.

If you want to know more about the ***Forward-Backward*** procedure of calculating alphas and betas,
check out Jason Eisner’s tutorial ([Eisner, 2002](#refEisner)), including a spreadsheet with the changing values.
The second edition of the AI handbook ([Russell/Norvig, 2003, 724 – 733](refRN)) has a comprehensive
section about EM.

Kevin Knight has written a very compelling ***introduction*** ([Knight, 2009a](#refKnightA)). It is mainly about
Bayesian Inference, but explains EM very nicely. You might also want to check out his tutorials
for Carmel ([Knight, 2009b](#refKnightB)), a software that helps you implement graphical models as automata and
train them with EM. Also, the workbook for MT ([Knight, 1999](#refKnightMT)) contains a useful section on EM.

[Vaswani/Pauls/Chiang 2010](#refVaswani) show how using ***L$_0$ normalization*** can lead to smaller models and a L0
sparser distribution, which improves language related tasks a lot, because it creates a more Zipfian normalization
distribution (see [Hovy et al. 2011](#refHovy) for another application).

<a name="secadvanced"></a>9 Advanced Topics
==

The goal of his section is mainly to give you an idea of some of the topics are out there to address
problems with EM. It would lead too far to go into detail for each, so we will give a very high-level
intuition and refer to related work for details.


9.1 Scaling
--

In the examples above, we have used logarithms to prevent underflow of the probabilities. That turned
all our multiplications into additions (which is good, because it’s fast), but it also turned every addition
into a function call to a log addition (which unfortunately is very slow). There is another way to prevent
underflow, called ***scaling***, but it requires some more bookkeeping. The idea is that we normalize each 
column in our lattice so that it sums to 1. That way, the probabilities won’t get too small, and we can
still use multiplication and normal addition.

Practically, we add a vector to our forward-backward procedure. It has the same length as out lattice,
and for each position of the lattice contains the sum of the forward probabilities at that position, the
***scaling factor***. Before we move on, we normalize the column by that factor, so that it sums to 1. The 
backward pass uses the same scaling parameters as the forward pass.
We can then use the scaled forward and backward matrices as well as the scaling factor vector to
compute the fractional counts. See [Shen (2008)](#refShen) for more details (abstract mathematical notation, but
very good explanations).


9.2 Variational Bayes
--

We have earlier seen the idea of adding pseudo-counts to the fractional counts, which helps with
smoothing. Variational Bayes inference works similarly, only that now we define a prior for our prior
parameters.

A ***prior*** is essentially a curve that looks similar to what we would like to achieve (see the example
above of how EM’s distribution differs from the real one: a prior would look like the real one). In
practice, the prior here is a ***Dirichlet distribution***, which takes two parameters: a probability distribution and a vector of shape parameters. Both have the same number of elements (or dimensions). If 
we have only two dimensions in each, the prior is called a Beta function. Typically, the probability
distribution are our transition parameters, and the shape parameters are something like pseudo-counts
for each element.

This sounds rather complicated, but is relatively easy to implement. The E-step stays the same
as before. In the M-step, the elements of the shape parameter vector are added as pseudo-counts
to the matching fractional counts, and the result is passed through a ***Digamma function*** and finally
exponentiated. To normalize, we add the sum of all elements in the shape vector to our denominator
and again run it though Digamma and exponentiation. This is like a softer version of smoothing with
pseudo-counts, where we have separate counts for each parameter.
